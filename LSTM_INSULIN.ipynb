{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lca-mgce8U7A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "import glob\n",
        "import os\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set your data path\n",
        "DATA_PATH = \"/content/drive/MyDrive/Colab Notebooks/Preprocessed/\"\n",
        "\n",
        "# Check all files in the directory\n",
        "import os\n",
        "all_files = os.listdir(DATA_PATH)\n",
        "print(\"All files in directory:\")\n",
        "for f in all_files:\n",
        "    print(f\" - {f}\")\n",
        "\n",
        "# Filter for CSV files\n",
        "csv_files = [f for f in all_files if f.endswith('.csv')]\n",
        "print(f\"\\nFound {len(csv_files)} CSV files:\")\n",
        "for f in csv_files:\n",
        "    print(f\" - {f}\")"
      ],
      "metadata": {
        "id": "tFd7SvMj5aw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_all_patients_data(data_path, csv_files):\n",
        "\n",
        "    patients_data = {}\n",
        "\n",
        "    for filename in csv_files:\n",
        "        try:\n",
        "            filepath = os.path.join(data_path, filename)\n",
        "\n",
        "            # Extract patient number from filename\n",
        "            patient_num_str = filename.replace('HUPA', '').replace('P.csv', '').replace('.csv', '').lstrip('0')\n",
        "            patient_num = int(patient_num_str) if patient_num_str else 1\n",
        "\n",
        "            # Load with proper semicolon separation\n",
        "            df = pd.read_csv(filepath, sep=';')\n",
        "\n",
        "\n",
        "            # Convert time column to datetime\n",
        "            df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
        "\n",
        "            # Drop rows with invalid time\n",
        "            df = df.dropna(subset=['time'])\n",
        "\n",
        "            # Sort by time\n",
        "            df = df.sort_values('time').reset_index(drop=True)\n",
        "\n",
        "            # Convert numeric columns to appropriate types\n",
        "            numeric_columns = ['glucose', 'calories', 'heart_rate', 'steps',\n",
        "                             'basal_rate', 'bolus_volume_delivered', 'carb_input']\n",
        "\n",
        "            for col in numeric_columns:\n",
        "                if col in df.columns:\n",
        "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "            patients_data[patient_num] = df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {filename}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"\\nSuccessfully loaded {len(patients_data)} patients\")\n",
        "    return patients_data\n",
        "\n",
        "# Load all patient data with proper semicolon separation\n",
        "all_patients = load_all_patients_data(DATA_PATH, csv_files)"
      ],
      "metadata": {
        "id": "Zhd3jVP95ZNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if all_patients:\n",
        "    first_patient_id = list(all_patients.keys())[0]\n",
        "    first_patient_data = all_patients[first_patient_id]\n",
        "\n",
        "    print(\"First patient data shape:\", first_patient_data.shape)\n",
        "    print(\"\\nColumns:\", first_patient_data.columns.tolist())\n",
        "    print(\"\\nFirst few rows:\")\n",
        "    display(first_patient_data.head(10))\n",
        "\n",
        "    # Data info\n",
        "    print(\"\\nData types:\")\n",
        "    print(first_patient_data.dtypes)\n",
        "\n",
        "    # Check for missing values\n",
        "    print(\"\\nMissing values per column:\")\n",
        "    print(first_patient_data.isnull().sum())\n",
        "\n",
        "    # Check time range and frequency\n",
        "    print(f\"\\nTime range: {first_patient_data['time'].min()} to {first_patient_data['time'].max()}\")\n",
        "    time_diff = first_patient_data['time'].diff().mean()\n",
        "    print(f\"Average time difference: {time_diff}\")\n",
        "\n",
        "    # Basic statistics\n",
        "    print(\"\\nBasic statistics:\")\n",
        "    display(first_patient_data.describe())\n",
        "\n",
        "else:\n",
        "    print(\"No patients loaded. Let's debug the file structure...\")\n",
        "\n",
        "    # Debug: Check raw file content\n",
        "    if csv_files:\n",
        "        sample_file = os.path.join(DATA_PATH, csv_files[0])\n",
        "        print(f\"\\nDebugging file: {csv_files[0]}\")\n",
        "\n",
        "        # Read raw file content\n",
        "        with open(sample_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        print(\"First 3 lines of raw file:\")\n",
        "        for i, line in enumerate(lines[:3]):\n",
        "            print(f\"Line {i}: {line.strip()}\")"
      ],
      "metadata": {
        "id": "wLho4Uc15ffr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if all_patients:\n",
        "    print(\"Data Quality Summary:\")\n",
        "\n",
        "\n",
        "    for patient_id, data in list(all_patients.items())[:5]:  # Check first 5 patients\n",
        "        print(f\"\\nPatient {patient_id}:\")\n",
        "        print(f\"  Records: {len(data)}\")\n",
        "        print(f\"  Time span: {data['time'].max() - data['time'].min()}\")\n",
        "        print(f\"  Glucose range: {data['glucose'].min():.1f} - {data['glucose'].max():.1f}\")\n",
        "        print(f\"  Missing values: {data.isnull().sum().sum()}\")\n",
        "\n",
        "        # Check if we have insulin delivery data\n",
        "        if 'bolus_volume_delivered' in data.columns:\n",
        "            bolus_records = data[data['bolus_volume_delivered'] > 0]\n",
        "            print(f\"  Bolus records: {len(bolus_records)}\")\n",
        "\n",
        "        if 'carb_input' in data.columns:\n",
        "            carb_records = data[data['carb_input'] > 0]\n",
        "            print(f\"  Carb intake records: {len(carb_records)}\")"
      ],
      "metadata": {
        "id": "7KaUdQW75jwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_patient_data(patient_df, target_glucose=110, insulin_sensitivity_factor=50):\n",
        "\n",
        "    df = patient_df.copy()\n",
        "\n",
        "    # Ensure numeric types\n",
        "    numeric_columns = ['glucose', 'calories', 'heart_rate', 'steps',\n",
        "                      'basal_rate', 'bolus_volume_delivered', 'carb_input']\n",
        "\n",
        "    for col in numeric_columns:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Feature engineering\n",
        "    df['glucose_change'] = df['glucose'].diff()  # Rate of change\n",
        "    df['glucose_rolling_mean'] = df['glucose'].rolling(window=3, min_periods=1).mean()\n",
        "    df['calories_cumulative'] = df['calories'].rolling(window=6, min_periods=1).sum()\n",
        "    df['steps_cumulative'] = df['steps'].rolling(window=6, min_periods=1).sum()\n",
        "\n",
        "    # Time-based features\n",
        "    df['hour'] = df['time'].dt.hour\n",
        "    df['minute'] = df['time'].dt.minute\n",
        "    df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
        "    df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
        "\n",
        "    # Calculate insulin need target (simplified correction dose)\n",
        "    # Using standard correction formula: (Current Glucose - Target) / ISF\n",
        "    df['insulin_need'] = np.maximum(0, (df['glucose'] - target_glucose)) / insulin_sensitivity_factor\n",
        "\n",
        "    # Use future insulin need as target (shifted by 1 time step = 5 minutes)\n",
        "    df['target_insulin_need'] = df['insulin_need'].shift(-1)\n",
        "\n",
        "    # Drop NaN values created by shifting and rolling\n",
        "    df = df.dropna()\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_sequences_for_patient(patient_data, sequence_length=6):\n",
        "    \"\"\"\n",
        "    Create sequences for a single patient\n",
        "    \"\"\"\n",
        "    # Selected features for the model\n",
        "    features = [\n",
        "        'glucose', 'calories', 'heart_rate', 'steps',\n",
        "        'basal_rate', 'bolus_volume_delivered', 'carb_input',\n",
        "        'glucose_change', 'glucose_rolling_mean',\n",
        "        'calories_cumulative', 'steps_cumulative',\n",
        "        'sin_hour', 'cos_hour'\n",
        "    ]\n",
        "\n",
        "    # Only use features that exist in the dataframe\n",
        "    available_features = [f for f in features if f in patient_data.columns]\n",
        "\n",
        "    X, y = [], []\n",
        "\n",
        "    for i in range(len(patient_data) - sequence_length):\n",
        "        # Input sequence (last 30 minutes = 6 time steps * 5min)\n",
        "        sequence_features = patient_data[available_features].iloc[i:i+sequence_length].values\n",
        "        X.append(sequence_features)\n",
        "\n",
        "        # Target: Insulin need in next period (after the sequence)\n",
        "        target = patient_data['target_insulin_need'].iloc[i+sequence_length-1]\n",
        "        y.append(target)\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "vZ7mx2_H5mMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the original simple data preparation function\n",
        "def prepare_all_patients_data(all_patients_data, sequence_length=6):\n",
        "\n",
        "    # Patient IDs\n",
        "    patient_ids = list(all_patients_data.keys())\n",
        "    np.random.seed(42)\n",
        "    np.random.shuffle(patient_ids)\n",
        "\n",
        "    # Split patients (20 train, 3 val, 2 test for 25 patients)\n",
        "    train_patients = patient_ids[:20]\n",
        "    val_patients = patient_ids[20:23]\n",
        "    test_patients = patient_ids[23:]\n",
        "\n",
        "    print(f\"Train patients: {train_patients}\")\n",
        "    print(f\"Val patients: {val_patients}\")\n",
        "    print(f\"Test patients: {test_patients}\")\n",
        "\n",
        "    X_train, y_train = [], []\n",
        "    X_val, y_val = [], []\n",
        "    X_test, y_test = [], []\n",
        "\n",
        "    # Process each patient\n",
        "    for patient_id, patient_data in all_patients_data.items():\n",
        "        # Preprocess patient data\n",
        "        processed_data = preprocess_patient_data(patient_data)\n",
        "\n",
        "        # Create sequences\n",
        "        X_patient, y_patient = create_sequences_for_patient(processed_data, sequence_length)\n",
        "\n",
        "        # Assign to appropriate split\n",
        "        if patient_id in train_patients:\n",
        "            X_train.append(X_patient)\n",
        "            y_train.append(y_patient)\n",
        "        elif patient_id in val_patients:\n",
        "            X_val.append(X_patient)\n",
        "            y_val.append(y_patient)\n",
        "        else:  # test patients\n",
        "            X_test.append(X_patient)\n",
        "            y_test.append(y_patient)\n",
        "\n",
        "    # Concatenate all sequences\n",
        "    if X_train:\n",
        "        X_train = np.concatenate(X_train, axis=0)\n",
        "        y_train = np.concatenate(y_train, axis=0)\n",
        "    if X_val:\n",
        "        X_val = np.concatenate(X_val, axis=0)\n",
        "        y_val = np.concatenate(y_val, axis=0)\n",
        "    if X_test:\n",
        "        X_test = np.concatenate(X_test, axis=0)\n",
        "        y_test = np.concatenate(y_test, axis=0)\n",
        "\n",
        "    print(f\"\\nFinal dataset shapes:\")\n",
        "    print(f\"Training set: {X_train.shape} sequences\")\n",
        "    print(f\"Validation set: {X_val.shape} sequences\")\n",
        "    print(f\"Test set: {X_test.shape} sequences\")\n",
        "\n",
        "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), (train_patients, val_patients, test_patients)\n",
        "\n",
        "# Now prepare the data with the simple version\n",
        "\n",
        "(X_train, y_train), (X_val, y_val), (X_test, y_test), patient_splits = prepare_all_patients_data(all_patients)\n",
        "\n",
        "print(f\"Data prepared!\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "\n",
        "# Get the number of features\n",
        "n_features = X_train.shape[2]\n",
        "print(f\"Number of features: {n_features}\")\n"
      ],
      "metadata": {
        "id": "2c-H_7CZ5tIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LeakyReLU\n",
        "def create_insulin_lstm_deep(sequence_length=6, n_features=13):\n",
        "    model = Sequential([\n",
        "        LSTM(128, return_sequences=True, input_shape=(sequence_length, n_features),\n",
        "             kernel_initializer='he_normal', recurrent_dropout=0.2),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        LSTM(64, return_sequences=True, kernel_initializer='he_normal', recurrent_dropout=0.2),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        LSTM(32, return_sequences=False, kernel_initializer='he_normal', recurrent_dropout=0.1),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Dense(64, kernel_initializer='he_normal'),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Dense(32, kernel_initializer='he_normal'),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        Dropout(0.1),\n",
        "\n",
        "        Dense(16, activation='relu', kernel_initializer='he_normal'),\n",
        "        Dense(1, activation='linear', name='insulin_output')\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "08NrUPFM5wHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the DEEP learning model\n",
        "model = create_insulin_lstm_deep(sequence_length=6, n_features=n_features)\n",
        "\n",
        "print(\"USING DEEP LEARNING MODEL!\")\n",
        "print(\"Model Architecture:\")\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Bhpf4wwF5yfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse',\n",
        "    metrics=['mae', 'mape']\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "save_path = '/content/drive/MyDrive/insulin_model/best_insulin_model.keras'\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model(save_path)\n",
        "print(\"Model loaded successfully!\")\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        min_delta=1e-4,\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=save_path,\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=32,\n",
        "    epochs=15,  # total additional epochs you want to run\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "ZVvSofFM52fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "D5un1Wz158Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Scatter: actual vs predicted\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.xlabel('Actual Insulin Need')\n",
        "plt.ylabel('Predicted Insulin Need')\n",
        "plt.title('Actual vs Predicted')\n",
        "\n",
        "# Line plot: first 100 points\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(y_test[:100], label='Actual', alpha=0.7)\n",
        "plt.plot(y_pred[:100], label='Predicted', alpha=0.7)\n",
        "plt.xlabel('Time Steps')\n",
        "plt.ylabel('Insulin Need')\n",
        "plt.title('Sample Predictions')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4WYeVpLE5-WF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}